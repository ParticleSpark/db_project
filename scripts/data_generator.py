"""
ç¤ºä¾‹æ€§èƒ½æ•°æ®ç”Ÿæˆå™¨
ç”¨äºå¿«é€Ÿä½“éªŒå¯è§†åŒ–åŠŸèƒ½ï¼Œæ— éœ€å®é™…è¿è¡Œæ•°æ®åº“æµ‹è¯•
"""

import pandas as pd
import numpy as np
import os

def generate_sample_data():
    """ç”Ÿæˆç¤ºä¾‹æ€§èƒ½æµ‹è¯•æ•°æ®"""
    
    np.random.seed(42)
    
    # å®šä¹‰æŸ¥è¯¢å’Œæ•°æ®åº“
    simple_queries = [f'Q{i}' for i in range(1, 9)]  # Q1-Q8
    complex_queries = [f'Q{i}' for i in range(1, 6)]  # Q1-Q5
    crud_operations = ['I1', 'D1', 'U1']
    
    databases = [
        'PostgreSQL',
        'PostgreSQL_indexed',
        'DuckDB',
        'DuckDB_indexed',
        'InfluxDB'
    ]
    
    data = []
    
    # ç”Ÿæˆç®€å•æŸ¥è¯¢æ•°æ®
    print("ç”Ÿæˆç®€å•æŸ¥è¯¢æ•°æ®...")
    for query in simple_queries:
        for db in databases:
            # åŸºå‡†æ—¶é—´ï¼ˆæ ¹æ®æ•°æ®åº“ç±»å‹è°ƒæ•´ï¼‰
            if db == 'PostgreSQL':
                base_time = np.random.uniform(100, 500)
            elif db == 'PostgreSQL_indexed':
                base_time = np.random.uniform(50, 200)
            elif db == 'DuckDB':
                base_time = np.random.uniform(40, 150)
            elif db == 'DuckDB_indexed':
                base_time = np.random.uniform(35, 140)
            else:  # InfluxDB
                base_time = np.random.uniform(150, 600)
            
            # è®¡ç®—å„éƒ¨åˆ†æ—¶é—´
            total_time = base_time * np.random.uniform(0.9, 1.1)
            query_time = total_time * np.random.uniform(0.6, 0.8)
            return_time = total_time - query_time
            rows = int(np.random.uniform(100, 10000))
            
            data.append({
                'query_name': query,
                'database': db,
                'execution_time_ms': round(total_time, 2),
                'query_time_ms': round(query_time, 2),
                'return_time_ms': round(return_time, 2),
                'rows_returned': rows,
                'query_type': 'simple'
            })
    
    # ç”Ÿæˆå¤æ‚æŸ¥è¯¢æ•°æ®
    print("ç”Ÿæˆå¤æ‚æŸ¥è¯¢æ•°æ®...")
    for query in complex_queries:
        for db in databases:
            if db == 'PostgreSQL':
                base_time = np.random.uniform(500, 2000)
            elif db == 'PostgreSQL_indexed':
                base_time = np.random.uniform(300, 1500)
            elif db == 'DuckDB':
                base_time = np.random.uniform(200, 1000)
            elif db == 'DuckDB_indexed':
                base_time = np.random.uniform(180, 950)
            else:  # InfluxDB
                base_time = np.random.uniform(800, 3000)
            
            total_time = base_time * np.random.uniform(0.9, 1.1)
            query_time = total_time * np.random.uniform(0.7, 0.85)
            return_time = total_time - query_time
            rows = int(np.random.uniform(1000, 50000))
            
            data.append({
                'query_name': query,
                'database': db,
                'execution_time_ms': round(total_time, 2),
                'query_time_ms': round(query_time, 2),
                'return_time_ms': round(return_time, 2),
                'rows_returned': rows,
                'query_type': 'complex'
            })
    
    # ç”ŸæˆCRUDæ“ä½œæ•°æ®
    print("ç”ŸæˆCRUDæ“ä½œæ•°æ®...")
    for operation in crud_operations:
        for db in databases:
            # InfluxDBä¸æ”¯æŒä¼ ç»ŸCRUDï¼Œè·³è¿‡
            if db == 'InfluxDB' and operation in ['D1', 'U1']:
                continue
            
            if db.startswith('PostgreSQL'):
                base_time = np.random.uniform(10, 50)
            else:  # DuckDB
                base_time = np.random.uniform(8, 40)
            
            if db == 'InfluxDB':
                base_time = np.random.uniform(15, 60)
            
            total_time = base_time * np.random.uniform(0.9, 1.1)
            query_time = total_time * np.random.uniform(0.85, 0.95)
            return_time = total_time - query_time
            rows = 1 if operation != 'I1' else int(np.random.uniform(1, 100))
            
            data.append({
                'query_name': operation,
                'database': db,
                'execution_time_ms': round(total_time, 2),
                'query_time_ms': round(query_time, 2),
                'return_time_ms': round(return_time, 2),
                'rows_returned': rows,
                'query_type': 'crud'
            })
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(data)
    
    # ä¿å­˜åˆ°dataç›®å½•
    output_path = os.path.join('data', 'sample_performance.csv')
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    
    print(f"\nâœ… ç¤ºä¾‹æ•°æ®å·²ç”Ÿæˆï¼")
    print(f"ğŸ“ ä¿å­˜ä½ç½®: {output_path}")
    print(f"ğŸ“Š æ•°æ®è¡Œæ•°: {len(df)}")
    print(f"ğŸ“‹ æŸ¥è¯¢ç±»å‹: {df['query_type'].unique()}")
    print(f"ğŸ’¾ æ•°æ®åº“ç±»å‹: {df['database'].unique()}")
    
    # æ˜¾ç¤ºæ•°æ®æ‘˜è¦
    print("\næ•°æ®æ‘˜è¦:")
    print(df.groupby(['query_type', 'database'])['execution_time_ms'].agg(['mean', 'min', 'max']).round(2))
    
    return df

if __name__ == "__main__":
    print("="*60)
    print("æ•°æ®åº“æ€§èƒ½æµ‹è¯• - ç¤ºä¾‹æ•°æ®ç”Ÿæˆå™¨")
    print("="*60)
    print()
    
    df = generate_sample_data()
    
    print("\n" + "="*60)
    print("æ•°æ®é¢„è§ˆï¼ˆå‰10è¡Œï¼‰:")
    print("="*60)
    print(df.head(10).to_string())
    
    print("\nâœ¨ ç°åœ¨å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹å¯è§†åŒ–ï¼š")
    print("   python scripts/visualize.py")
    print("   æˆ–")
    print("   streamlit run app.py")

