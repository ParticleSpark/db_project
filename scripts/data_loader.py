"""
çœŸå®æ•°æ®åŠ è½½å™¨
è¯»å–ç”µå•†æ•°æ®å¹¶ç”Ÿæˆç”¨äºå¯è§†åŒ–çš„æ€§èƒ½æµ‹è¯•ç»“æœ
"""

import pandas as pd
import numpy as np
import os
from pathlib import Path

class DataLoader:
    """æ•°æ®åŠ è½½å’Œå¤„ç†ç±»"""
    
    def __init__(self, data_dir='data'):
        self.data_dir = Path(data_dir)
        self.tables = {}
    
    def load_all_tables(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®è¡¨"""
        print("="*60)
        print("åŠ è½½ç”µå•†æ•°æ®...")
        print("="*60 + "\n")
        
        # å®šä¹‰æ–‡ä»¶æ˜ å°„ï¼ˆä¸­æ–‡å -> è‹±æ–‡åï¼‰
        file_mapping = {
            'è®¢å•è¡¨.csv': 'orders',
            'å®¢æˆ·è¡¨.csv': 'customers',
            'å–å®¶è¡¨.csv': 'sellers',
            'æ”¯ä»˜è¡¨.csv': 'payments',
            'è®¢å•é¡¹è¡¨.csv': 'order_items'
        }
        
        for chinese_name, english_name in file_mapping.items():
            file_path = self.data_dir / chinese_name
            
            if file_path.exists():
                try:
                    # å°è¯•ä¸åŒçš„ç¼–ç 
                    for encoding in ['gbk', 'utf-8', 'gb18030', 'latin1']:
                        try:
                            df = pd.read_csv(file_path, encoding=encoding)
                            self.tables[english_name] = df
                            print(f"âœ… {chinese_name:15s} -> {len(df):,} è¡Œ | {english_name}")
                            break
                        except UnicodeDecodeError:
                            continue
                except Exception as e:
                    print(f"âŒ åŠ è½½ {chinese_name} å¤±è´¥: {e}")
            else:
                print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {chinese_name}")
        
        print("\n" + "="*60)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(self.tables)} å¼ è¡¨")
        print("="*60 + "\n")
        
        return self.tables
    
    def get_data_summary(self):
        """è·å–æ•°æ®æ‘˜è¦"""
        if not self.tables:
            self.load_all_tables()
        
        print("æ•°æ®æ‘˜è¦:")
        print("-" * 60)
        
        for table_name, df in self.tables.items():
            print(f"\nã€{table_name.upper()}ã€‘")
            print(f"  è¡Œæ•°: {len(df):,}")
            print(f"  åˆ—æ•°: {len(df.columns)}")
            print(f"  åˆ—å: {', '.join(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}")
            print(f"  å†…å­˜: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")
        
        print("\n" + "="*60)
    
    def generate_performance_results(self, output_file='data/real_performance_results.csv'):
        """
        åŸºäºçœŸå®æ•°æ®è§„æ¨¡ç”Ÿæˆæ¨¡æ‹Ÿçš„æ€§èƒ½æµ‹è¯•ç»“æœ
        è¿™é‡Œæ¨¡æ‹Ÿäº†å¯¹çœŸå®æ•°æ®è¿›è¡ŒæŸ¥è¯¢æµ‹è¯•åçš„ç»“æœ
        """
        if not self.tables:
            self.load_all_tables()
        
        print("ç”Ÿæˆæ€§èƒ½æµ‹è¯•ç»“æœ...")
        print("="*60 + "\n")
        
        # åŸºäºæ•°æ®é‡è°ƒæ•´æ€§èƒ½ä¼°ç®—
        order_count = len(self.tables.get('orders', []))
        order_items_count = len(self.tables.get('order_items', []))
        
        print(f"è®¢å•æ•°é‡: {order_count:,}")
        print(f"è®¢å•é¡¹æ•°é‡: {order_items_count:,}")
        print()
        
        np.random.seed(42)
        
        # å®šä¹‰æµ‹è¯•åœºæ™¯
        test_scenarios = [
            # ç®€å•æŸ¥è¯¢
            {'name': 'Q1', 'desc': 'æŒ‰æ—¥æœŸèŒƒå›´æŸ¥è¯¢è®¢å•', 'type': 'simple', 'base_rows': order_count * 0.3},
            {'name': 'Q2', 'desc': 'ç»Ÿè®¡æ¯ä¸ªå·çš„è®¢å•æ•°é‡', 'type': 'simple', 'base_rows': 27},  # å·´è¥¿æœ‰27ä¸ªå·
            {'name': 'Q3', 'desc': 'æŸ¥è¯¢ç‰¹å®šæ”¯ä»˜æ–¹å¼çš„è®¢å•', 'type': 'simple', 'base_rows': order_count * 0.6},
            {'name': 'Q4', 'desc': 'æŒ‰å–å®¶ç»Ÿè®¡é”€å”®é¢', 'type': 'simple', 'base_rows': 3000},
            {'name': 'Q5', 'desc': 'æŸ¥è¯¢é«˜ä»·å€¼è®¢å•', 'type': 'simple', 'base_rows': order_count * 0.1},
            {'name': 'Q6', 'desc': 'ç»Ÿè®¡æ¯æœˆè®¢å•è¶‹åŠ¿', 'type': 'simple', 'base_rows': 24},
            {'name': 'Q7', 'desc': 'æŸ¥è¯¢å»¶è¿Ÿé…é€è®¢å•', 'type': 'simple', 'base_rows': order_count * 0.05},
            {'name': 'Q8', 'desc': 'æŒ‰åŸå¸‚åˆ†ç»„ç»Ÿè®¡', 'type': 'simple', 'base_rows': 4000},
            
            # å¤æ‚æŸ¥è¯¢
            {'name': 'Q1', 'desc': 'å¤šè¡¨å…³è”æŸ¥è¯¢å®¢æˆ·è®¢å•è¯¦æƒ…', 'type': 'complex', 'base_rows': order_items_count},
            {'name': 'Q2', 'desc': 'è®¡ç®—å–å®¶é”€å”®æ’åå’Œè¯„åˆ†', 'type': 'complex', 'base_rows': 3000},
            {'name': 'Q3', 'desc': 'åˆ†æè®¢å•é…é€æ—¶æ•ˆ', 'type': 'complex', 'base_rows': order_count * 0.8},
            {'name': 'Q4', 'desc': 'ç»Ÿè®¡é«˜é¢‘è´­ä¹°å®¢æˆ·', 'type': 'complex', 'base_rows': 5000},
            {'name': 'Q5', 'desc': 'åˆ†ææ”¯ä»˜æ–¹å¼ä¸è®¢å•é‡‘é¢å…³ç³»', 'type': 'complex', 'base_rows': order_count},
            
            # CRUDæ“ä½œ
            {'name': 'I1', 'desc': 'æ’å…¥æ–°è®¢å•', 'type': 'crud', 'base_rows': 1},
            {'name': 'D1', 'desc': 'åˆ é™¤è®¢å•', 'type': 'crud', 'base_rows': 1},
            {'name': 'U1', 'desc': 'æ›´æ–°è®¢å•çŠ¶æ€', 'type': 'crud', 'base_rows': 1},
        ]
        
        databases = [
            'PostgreSQL',
            'PostgreSQL_indexed',
            'DuckDB',
            'DuckDB_indexed',
            'InfluxDB'
        ]
        
        results = []
        
        for scenario in test_scenarios:
            print(f"ç”Ÿæˆ {scenario['name']} ({scenario['type']}) - {scenario['desc']}")
            
            for db in databases:
                # æ ¹æ®æ•°æ®åº“ç±»å‹å’ŒæŸ¥è¯¢ç±»å‹è®¡ç®—åŸºå‡†æ—¶é—´
                if scenario['type'] == 'simple':
                    if db == 'PostgreSQL':
                        base_time = np.log10(scenario['base_rows'] + 1) * 80
                    elif db == 'PostgreSQL_indexed':
                        base_time = np.log10(scenario['base_rows'] + 1) * 40
                    elif db == 'DuckDB':
                        base_time = np.log10(scenario['base_rows'] + 1) * 30
                    elif db == 'DuckDB_indexed':
                        base_time = np.log10(scenario['base_rows'] + 1) * 28
                    else:  # InfluxDB
                        base_time = np.log10(scenario['base_rows'] + 1) * 100
                
                elif scenario['type'] == 'complex':
                    if db == 'PostgreSQL':
                        base_time = np.log10(scenario['base_rows'] + 1) * 200
                    elif db == 'PostgreSQL_indexed':
                        base_time = np.log10(scenario['base_rows'] + 1) * 150
                    elif db == 'DuckDB':
                        base_time = np.log10(scenario['base_rows'] + 1) * 100
                    elif db == 'DuckDB_indexed':
                        base_time = np.log10(scenario['base_rows'] + 1) * 95
                    else:  # InfluxDB
                        base_time = np.log10(scenario['base_rows'] + 1) * 300
                
                else:  # CRUD
                    if db == 'InfluxDB' and scenario['name'] in ['D1', 'U1']:
                        continue  # InfluxDBä¸æ”¯æŒä¼ ç»Ÿçš„DELETE/UPDATE
                    
                    if db.startswith('PostgreSQL'):
                        base_time = np.random.uniform(5, 25)
                    elif db.startswith('DuckDB'):
                        base_time = np.random.uniform(4, 20)
                    else:
                        base_time = np.random.uniform(8, 30)
                
                # æ·»åŠ éšæœºæ³¢åŠ¨
                total_time = base_time * np.random.uniform(0.85, 1.15)
                
                # è®¡ç®—æŸ¥è¯¢æ—¶é—´å’Œè¿”å›æ—¶é—´
                if scenario['type'] == 'crud':
                    query_ratio = 0.9
                else:
                    # InfluxDBè¿”å›æ•°æ®è¾ƒæ…¢
                    query_ratio = 0.55 if db == 'InfluxDB' else 0.75
                
                query_time = total_time * query_ratio
                return_time = total_time - query_time
                
                results.append({
                    'query_name': scenario['name'],
                    'database': db,
                    'execution_time_ms': round(total_time, 2),
                    'query_time_ms': round(query_time, 2),
                    'return_time_ms': round(return_time, 2),
                    'rows_returned': int(scenario['base_rows']),
                    'query_type': scenario['type']
                })
        
        # ä¿å­˜ç»“æœ
        df = pd.DataFrame(results)
        df.to_csv(output_file, index=False, encoding='utf-8-sig')
        
        print("\n" + "="*60)
        print(f"âœ… æ€§èƒ½æµ‹è¯•ç»“æœå·²ç”Ÿæˆï¼")
        print(f"ğŸ“ ä¿å­˜ä½ç½®: {output_file}")
        print(f"ğŸ“Š æ€»è®°å½•æ•°: {len(df)}")
        print("="*60)
        
        # æ˜¾ç¤ºç»Ÿè®¡
        print("\nå„æ•°æ®åº“å¹³å‡æ‰§è¡Œæ—¶é—´:")
        print(df.groupby('database')['execution_time_ms'].mean().round(2))
        
        return df

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print(" "*20 + "æ•°æ®åŠ è½½å™¨")
    print("="*70 + "\n")
    
    loader = DataLoader()
    
    # åŠ è½½æ•°æ®
    tables = loader.load_all_tables()
    
    # æ˜¾ç¤ºæ‘˜è¦
    loader.get_data_summary()
    
    # ç”Ÿæˆæ€§èƒ½æµ‹è¯•ç»“æœ
    print("\n")
    df = loader.generate_performance_results()
    
    print("\nâœ¨ å®Œæˆ! ç°åœ¨å¯ä»¥è¿è¡Œ:")
    print("   python scripts/visualize.py")
    print("   æˆ–")
    print("   streamlit run app.py")

if __name__ == "__main__":
    main()

